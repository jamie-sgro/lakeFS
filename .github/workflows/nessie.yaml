name: Nessie
on:
  workflow_run:
    workflows: ["Prepare image for tests"]
    types:
      - completed
jobs:
  extract-version:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Extract version
        shell: bash
        run: echo "::set-output name=tag::sha-$(git rev-parse --short HEAD | sed s/^v//g)"
        id: version
    runs-on: ubuntu-20.04
    outputs:
      tag: ${{ steps.version.outputs.tag }}
  spark2:
    name: Test lakeFS with Spark 2.x
    needs: extract-version
    runs-on: ubuntu-20.04
    env:
      TAG: ${{ needs.extract-version.outputs.tag }}
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
      SPARK_TAG: 2
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Check-out code
        uses: actions/checkout@v2

      - name: Setup Scala
        uses: olafurpg/setup-scala@v10

      - name: Package Spark App
        working-directory: test/spark/app
        run: sbt sonnets-246/package

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Start lakeFS S3 and Spark 2.x
        env:
          LAKEFS_STATS_ENABLED: "false"
          LAKEFS_BLOCKSTORE_TYPE: s3
          LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.docker.lakefs.io:8000
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
        working-directory: test/spark
        run: docker-compose up -d

      - name: Setup lakeFS for tests
        working-directory: test/spark
        run: ./setup-test.sh

      - name: Test lakeFS S3 with Spark 2.x
        env:
          STORAGE_NAMESPACE: s3://nessie-system-testing/${{ github.run_number }}-spark
          REPOSITORY: gateway-test
          SONNET_JAR: sonnets-246/target/sonnets-246/scala-2.11/sonnets-246_2.11-0.1.0.jar
        working-directory: test/spark
        run: ./run-test.sh

      - name: lakeFS Logs on Spark with gateway failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

      - name: Build Spark direct-access client
        working-directory: clients/hadoopfs
        run: mvn -Passembly -Djar.finalName=client --batch-mode --update-snapshots package

      - name: Test lakeFS S3 with Spark 2.x thick client
        timeout-minutes: 2
        env:
          JARS: clients/hadoopfs/
          STORAGE_NAMESPACE: s3://nessie-system-testing/${{ github.run_number }}-spark2-client
          AWS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
          USE_DIRECT_ACCESS: "true"
          REPOSITORY: thick-client-test
          SONNET_JAR: sonnets-246/target/sonnets-246/scala-2.11/sonnets-246_2.11-0.1.0.jar
        working-directory: test/spark
        run: ./run-test.sh

      - name: lakeFS Logs on Spark 2.x with client failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

  spark3:
    name: Test lakeFS with Spark 3.x
    needs: extract-version
    runs-on: ubuntu-20.04
    env:
      TAG: ${{ needs.extract-version.outputs.tag }}
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
      SPARK_TAG: 3
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Check-out code
        uses: actions/checkout@v2

      - name: Setup Scala
        uses: olafurpg/setup-scala@v10

      - name: Package Spark App
        working-directory: test/spark/app
        run: sbt sonnets-311/package

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Start lakeFS S3 and Spark 3.x
        env:
          LAKEFS_STATS_ENABLED: "false"
          LAKEFS_BLOCKSTORE_TYPE: s3
          LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.docker.lakefs.io:8000
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
        working-directory: test/spark
        run: docker-compose up -d

      - name: Setup lakeFS for tests
        working-directory: test/spark
        run: ./setup-test.sh

      - name: Test lakeFS S3 with Spark 3.x
        env:
          STORAGE_NAMESPACE: s3://nessie-system-testing/${{ github.run_number }}-spark
          REPOSITORY: gateway-test
          SONNET_JAR: sonnets-311/target/sonnets-311/scala-2.12/sonnets-311_2.12-0.1.0.jar
        working-directory: test/spark
        run: ./run-test.sh

      - name: lakeFS Logs on Spark with gateway failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

      - name: Build Spark direct-access client
        working-directory: clients/hadoopfs
        run: mvn -Passembly -Djar.finalName=client --batch-mode --update-snapshots package

      - name: Test lakeFS S3 with Spark 3.x thick client
        timeout-minutes: 2
        env:
          JARS: clients/hadoopfs/
          STORAGE_NAMESPACE: s3://nessie-system-testing/${{ github.run_number }}-spark3-client
          AWS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
          USE_DIRECT_ACCESS: "true"
          REPOSITORY: thick-client-test
          SONNET_JAR: sonnets-311/target/sonnets-311/scala-2.12/sonnets-311_2.12-0.1.0.jar
        working-directory: test/spark
        run: ./run-test.sh

      - name: lakeFS Logs on Spark with client failure
        if: ${{ failure() }}
        continue-on-error: true
        working-directory: test/spark
        run: docker-compose logs --tail=1000 lakefs

  run-system-aws-s3:
    name: Run latest lakeFS app on AWS S3
    needs: [gen-code, extract-version]
    runs-on: ubuntu-20.04
    env:
      TAG: ${{ needs.extract-version.outputs.tag }}
      # Setting Account_ID as a secret as a way to avoid specifying it here
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Retrieve generated code
        uses: actions/download-artifact@v2
        with:
          name: generated-code
          path: /tmp/
      - name: Unpack generated code
        run: tar -xf /tmp/generated.tar.gz
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
      - name: Run lakeFS S3
        env:
          LAKEFS_STATS_ENABLED: "false"
          LAKEFS_BLOCKSTORE_TYPE: s3
          LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.local.lakefs.io:8000
          DOCKER_REG: ${{ steps.login-ecr.outputs.registry }}
          AWS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
          NESSIE_TEST_DATA_ACCESS: true,false
          NESSIE_STORAGE_NAMESPACE: s3://nessie-system-testing/${{ github.run_number }}
          NESSIE_AWS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          NESSIE_AWS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
        run: docker-compose -f nessie/ops/docker-compose.yaml up --quiet-pull --exit-code-from=nessie
      - name: Check files in S3 bucket
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
        run: |
            FILES_COUNT=`aws s3 ls s3://nessie-system-testing/${{ github.run_number }} --recursive | wc -l`
            [ $FILES_COUNT -gt 5 ]
      - name: lakeFS Logs on s3 failure
        if: ${{ failure() }}
        continue-on-error: true
        run: docker-compose -f nessie/ops/docker-compose.yaml logs --tail=1000 lakefs
      - name: Export DB
        if: ${{ always() }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
        run: |
            cd nessie/ops
            docker-compose ps -q postgres && docker-compose exec -T postgres pg_dumpall --username=lakefs | gzip | aws s3 cp - s3://nessie-system-testing/${{ github.run_number }}/dump.gz
      - name: Run lakeFS S3 to use with local API key
        env:
          LAKEFS_STATS_ENABLED: "false"
          LAKEFS_BLOCKSTORE_TYPE: s3
          LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.local.lakefs.io:8000
          DOCKER_REG: ${{ steps.login-ecr.outputs.registry }}
          AWS_ACCESS_KEY_ID: ${{ secrets.NESSIE_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.NESSIE_AWS_SECRET_ACCESS_KEY }}
          NESSIE_TEST_DATA_ACCESS: true,false
          NESSIE_STORAGE_NAMESPACE: s3://nessie-system-testing/${{ github.run_number }}-local-api-key
        run: |
          docker-compose -f nessie/ops/docker-compose.yaml down -v
          docker-compose -f nessie/ops/docker-compose.yaml up --quiet-pull --exit-code-from=nessie

  run-system-gcp-gs:
    name: Run latest lakeFS app on Google Cloud Platform and Google Cloud Storage
    needs: [gen-code, extract-version]
    runs-on: ubuntu-20.04
    env:
      TAG: ${{ needs.extract-version.outputs.tag }}
      # Setting Account_ID as a secret as a way to avoid specifying it here
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Retrieve generated code
        uses: actions/download-artifact@v2
        with:
          name: generated-code
          path: /tmp/
      - name: Unpack generated code
        run: tar -xf /tmp/generated.tar.gz
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
      - name: Run lakeFS GS
        env:
          LAKEFS_STATS_ENABLED: "false"
          LAKEFS_BLOCKSTORE_TYPE: gs
          LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.local.lakefs.io:8000
          DOCKER_REG: ${{ steps.login-ecr.outputs.registry }}
          AWS_ACCESS_KEY_ID: ""
          AWS_SECRET_ACCESS_KEY: ""
          LAKEFS_BLOCKSTORE_GS_CREDENTIALS_JSON: ${{ secrets.LAKEFS_BLOCKSTORE_GS_CREDENTIALS_JSON }}
          NESSIE_STORAGE_NAMESPACE: gs://nessie-system-testing/${{ github.run_number }}
        run: |
            docker-compose -f nessie/ops/docker-compose.yaml down -v
            docker-compose -f nessie/ops/docker-compose.yaml up --quiet-pull --exit-code-from=nessie
      - name: lakeFS Logs on GS failure
        if: ${{ failure() }}
        continue-on-error: true
        run: docker-compose -f nessie/ops/docker-compose.yaml logs --tail=1000 lakefs

  run-system-azure-abfs:
    name: Run latest lakeFS app on Azure with Azure blobstore
    needs: [gen-code, extract-version]
    runs-on: ubuntu-20.04
    env:
      TAG: ${{ needs.extract-version.outputs.tag }}
      # Setting Account_ID as a secret as a way to avoid specifying it here
      REPO: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.us-east-1.amazonaws.com
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Retrieve generated code
        uses: actions/download-artifact@v2
        with:
          name: generated-code
          path: /tmp/
      - name: Unpack generated code
        run: tar -xf /tmp/generated.tar.gz
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Run lakeFS Azure
        env:
          LAKEFS_STATS_ENABLED: "false"
          LAKEFS_BLOCKSTORE_TYPE: azure
          LAKEFS_GATEWAYS_S3_DOMAIN_NAME: s3.local.lakefs.io:8000
          DOCKER_REG: ${{ steps.login-ecr.outputs.registry }}
          AWS_ACCESS_KEY_ID: ""
          AWS_SECRET_ACCESS_KEY: ""
          LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT: ${{ secrets.LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT }}
          LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY: ${{ secrets.LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCESS_KEY }}
          NESSIE_STORAGE_NAMESPACE: https://${{ secrets.LAKEFS_BLOCKSTORE_AZURE_STORAGE_ACCOUNT }}.blob.core.windows.net/nessie-system-testing/${{ github.run_number }}
        run: |
          docker-compose -f nessie/ops/docker-compose.yaml down -v
          docker-compose -f nessie/ops/docker-compose.yaml up --quiet-pull --exit-code-from=nessie
      - name: lakeFS Logs on Azure failure
        if: ${{ failure() }}
        continue-on-error: true
        run: docker-compose -f nessie/ops/docker-compose.yaml logs --tail=1000 lakefs
      - name: See the env when we would have tried to publish coverage
        run: env
        # uses: codecov/codecov-action@v1
        # with:
        #   files: ./nessie-cover.out
        #   fail_ci_if_error: false
